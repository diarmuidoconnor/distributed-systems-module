## S3->SQS->Lambda.

Suppose we want to trigger a lambda function when an image file is uploaded to the S3 bucket, but still limit the number of concurrent running instances of the function when the upload frequency is high. A solution to satisfy this scenario is to insert an SQS queue between the bucket and lambda function. The SQS service logs the upload events generated by S3, and batches them when the Lambda service polls SQS. The batch processing reduces the number of concurrent instances of the lambda function. 

![][arch1]

In `lib/eda-app-stack.ts`, place the following code after the bucket creation:
~~~ts
  // Integration infrastructure

  const queue = new sqs.Queue(this, "img-uploadeded-q", {
      receiveMessageWaitTime: cdk.Duration.seconds(5),
    });

  // Lambda functions

    const processImageFn = new lambdanode.NodejsFunction(
      this,
      "ProcessImage",
      {
        runtime: lambda.Runtime.NODEJS_18_X,
        entry: `${__dirname}/../lambdas/processImage.ts`,
        timeout: cdk.Duration.seconds(15),
        memorySize: 128,
      }
    );

    // S3 --> SQS
    imagesBucket.addEventNotification(
      s3.EventType.OBJECT_CREATED,
      new s3n.SqsDestination(queue)
    );

   // SQS --> Lambda
    const newImageEventSource = new events.SqsEventSource(queue, {
      batchSize: 5,
      maxBatchingWindow: cdk.Duration.seconds(5),
    });

    processImageFn.addEventSource(newImageEventSource);

    // Permissions

    imagesBucket.grantRead(processImageFn);

    // Output
    
    new cdk.CfnOutput(this, "bucketName", {
      value: imagesBucket.bucketName,
    });
~~~
The code above configures the S3 service to notify the SQS service when an object is uploaded, and SQS logs each one in the queue. The queue is the event source (trigger) for the lambda function. The event received by the function will contain a batch of SQS messages related to uploads. Each messages includes the identity of the uploaded objects, enabling the lambda function to access it, if necessary.

Create the file `lambdas/processImage.ts` and add the following code:
~~~ts
/* eslint-disable import/extensions, import/no-absolute-path */
import { SQSHandler } from "aws-lambda";
import {
  GetObjectCommand,
  GetObjectCommandInput,
  S3Client,
} from "@aws-sdk/client-s3";

const s3 = new S3Client();

export const handler: SQSHandler = async (event) => {
  console.log("Event ", JSON.stringify(event));
  for (const record of event.Records) {
    const recordBody = JSON.parse(record.body);
    if (recordBody.Records) {
      console.log("Record body ", JSON.stringify(recordBody));
      for (const messageRecord of recordBody.Records) {
        const s3e = messageRecord.s3;
        const srcBucket = s3e.bucket.name;
        // Object key may have spaces or unicode non-ASCII characters.
        const srcKey = decodeURIComponent(s3e.object.key.replace(/\+/g, " "));
        let theImage = null;
        try {
          // Download the image from the S3                        bucket.
          const params: GetObjectCommandInput = {
            Bucket: srcBucket,
            Key: srcKey,
          };
          theImage = await s3.send(new GetObjectCommand(params));
          // We can process the image here if required......
        } catch (error) {
          console.log(error);
        }
      }
    }
  }
};
~~~
The above function contains nested for-loop constructs. The outer loop iterates over the batch of messages received from SQS; the inner loop deals with the possibility (very rare) of multiple file upload events in a single SQS message. For illustration purposes, the function downloads each file locally for possible processing (not relevant here).

Deploy the updated stack:
~~~bash
$ cdk deploy
~~~
To test the stack, upload an image using the AWS CLI:
~~~bash
$ aws s3 cp ./images/sunflower.jpeg  s3://your_bucket_name/image1.jpeg
e.g.
$ aws s3 cp ./images/sunflower.jpeg  s3://EDAStack-images9bf4dcd5-tc5q9f314rn6/image1.jpeg
~~~
In the management console, go to CloudWatch --> Log Groups --> Select the group for /aws/lambda/EDAStack-ProcessImageFnxxxxx --> Select the most recent log stream. Notice in the log output of the event psrameter, the body property of each SQS message is a stringified JSON structure, which must be parsed before processing:

![][event]

Each SQS message is comprised of an arrar of S3 messages (array of image uploads, potentially) - usually this array has only one element. Each S3 message includes identification details of the file uploaded, as shown below:

![][message]

Commit this work:
~~~bash
$ git add -A
$ git commit -m "Signup resource"
$ git push origin master
~~~

[arch1]: ./img/arch1.png
[event]: ./img/event.png
[message]: ./img/message.png

[pathparameters]: ./img/pathparameters.png

