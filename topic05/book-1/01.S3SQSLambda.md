## S3->SQS->Lambda.

Suppose we want to trigger a lambda function when an image file is uploaded to the S3 bucket. However, we want to limit the number of concurrent instances of the function when the upload count is high. A solution is to use an SQS queue to batch the upload event messages before triggering the function instance.

![][arch1]

In `lib/eda-app-stack.ts`, add the following code after the bucket creation:
~~~ts
    // Integration infrastructure

  const queue = new sqs.Queue(this, "img-created-queue", {
      receiveMessageWaitTime: cdk.Duration.seconds(10),
    });

    // Lambda functions

    const processImageFn = new lambdanode.NodejsFunction(
      this,
      "ProcessImageFn",
      {
        runtime: lambda.Runtime.NODEJS_18_X,
        entry: `${__dirname}/../lambdas/processImage.ts`,
        timeout: cdk.Duration.seconds(15),
        memorySize: 128,
      }
    );

   // S3 --> SQS
    imagesBucket.addEventNotification(
      s3.EventType.OBJECT_CREATED,
      new s3n.SqsDestination(queue)
    );

   // SQS --> Lambda
    const newImageEventSource = new events.SqsEventSource(queue, {
      batchSize: 5,
      maxBatchingWindow: cdk.Duration.seconds(10),
    });

    processImageFn.addEventSource(newImageEventSource);

    // Permissions

    imagesBucket.grantRead(processImageFn);

    // Output
    
    new cdk.CfnOutput(this, "bucketName", {
      value: imagesBucket.bucketName,
    });
~~~
The code above configures the S3 bucket to notify an SQS queue when the user uploads new objects (files). This queue is attached to a lambda function consumer, which results in the lambda handler receiving batches of messages relating to file uploads. The messages include the identity details of the uploaded objects, enabling the lambda function to access the image files, if  necessary. We must also grant the function read permission to the bucket.

Create the file `lambdas/processImage.ts` and add the following code:
~~~ts
/* eslint-disable import/extensions, import/no-absolute-path */
import { SQSHandler } from "aws-lambda";
import {
  GetObjectCommand,
  GetObjectCommandInput,
  S3Client,
} from "@aws-sdk/client-s3";

const s3 = new S3Client();

export const handler: SQSHandler = async (event) => {
  console.log("Event ", event);
  for (const record of event.Records) {
    const recordBody = JSON.parse(record.body);
    if (recordBody.Records) {
      console.log("Record body ", JSON.stringify(recordBody));
      for (const messageRecord of recordBody.Records) {
        const s3e = messageRecord.s3;
        const srcBucket = s3e.bucket.name;
        // Object key may have spaces or unicode non-ASCII characters.
        const srcKey = decodeURIComponent(s3e.object.key.replace(/\+/g, " "));
        let origimage = null;
        try {
          // Download the image from the S3 source bucket.
          const params: GetObjectCommandInput = {
            Bucket: srcBucket,
            Key: srcKey,
          };
          origimage = await s3.send(new GetObjectCommand(params));
          // Process the image ......
        } catch (error) {
          console.log(error);
        }
      }
    }
  }
};
~~~
The above function contains nested for-loop constructs. The outer for-loop iterates over the batch of messages received from SQS and the inner loop deals with the possibility (very rare) of multiple file upload events in a single SQS message. For illustration purposes, the function downloads each file from the bucket before processing it locally (the processing detail is blank as it is not central to this lab's focus).

Deploy the updated stack:
~~~bash
$ cdk deploy
~~~
To test the stack, upload an image using the CLI:
~~~bash
$ aws s3 cp ./images/sunflower.jpeg  s3://your_bucket_name/image1.jpeg
e.g.
$ aws s3 cp ./images/sunflower.jpeg  s3://simpleappstack-images9bf4dcd5-tc5q9f314rn6/image1.jpeg
~~~
In the management console, go to CloudWatch --> Log Groups --> Select the group for /aws/lambda/EDAStack-ProcessImageFnxxxxx --> Select the newest log Stream. The event passed to the handler contains a batch (array) of messages retrieved from the SQS queue, and the body of each message is a stringified JSON structure, as illustrated below:

![][event]

The body of each message is also an array and represents a set of S3 events/messages - usually this array has only one element. Each message includes identification details of the file uploaded, as shown below:

![][message]

Commit this work:
~~~bash
$ git add -A
$ git commit -m "Signup resource"
$ git push origin master
~~~

[arch1]: ./img/arch1.png
[event]: ./img/event.png
[message]: ./img/message.png

[pathparameters]: ./img/pathparameters.png

